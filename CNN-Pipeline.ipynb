{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Required\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion():\n",
    "    \n",
    "    # Load data (already split into train/test)\n",
    "    # X-train: (60000, 28, 28), y-train: (60000,) - 60000 samples of 28x28px images and their labels\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # CNNs expects 4D input: (samples, height, width, channels)\n",
    "    # Expand channel dimension to match Keras CNN expectation - grayscale images have 1 channel\n",
    "    # (60000, 28, 28) -> (60000, 28, 28, 1)\n",
    "    X_train = np.expand_dims(X_train, axis=-1).astype('float32')\n",
    "    X_test  = np.expand_dims(X_test, axis=-1).astype('float32')\n",
    "    \n",
    "    # Normalize (Optional)\n",
    "    X_train /= 255.0\n",
    "    X_test  /= 255.0\n",
    "\n",
    "    input_shape = (28, 28, 1) # Height, Width, Channels\n",
    "\n",
    "    # One-hot encode labels - convert class vectors (0-9) to binary class vectors\n",
    "    num_classes = len(np.unique(y_train))                               # 10 (digits 0–9)\n",
    "    y_train = to_categorical(y_train, num_classes).astype('float32')    # Ensure dtype is float32\n",
    "    y_test  = to_categorical(y_test, num_classes).astype('float32')  \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, input_shape, num_classes\n",
    "\n",
    "X_train, y_train, X_test, y_test, input_shape, num_classes = data_ingestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, num_classes, filters1=32, filters2=64, dense_units=120, lr=0.001, optimizer_type='adam'):\n",
    "    \n",
    "    K.clear_session()  # erase previous model from memory\n",
    "    # Builds a LeNet-style CNN (ReLU + MaxPooling)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer 1: Conv -> ReLU -> MaxPool\n",
    "    # Conv2D(filters1, 5x5) - Learn basic patterns\n",
    "    # MaxPooling2D - Reduce spatial size, keep strongest features\n",
    "    model.add(Conv2D(filters=filters1, kernel_size=(5,5), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # Layer 2: Conv -> ReLU -> MaxPool\n",
    "    # Conv2D(filters2, 5x5) - Learn more complex patterns\n",
    "    # MaxPooling2D - Further reduce spatial size\n",
    "    model.add(Conv2D(filters=filters2, kernel_size=(5,5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # Flatten before Dense layers - convert 2D feature maps to 1D feature vectors\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Classification layers\n",
    "    model.add(Dense(dense_units, activation='relu'))    # Dense(dense_units, ReLU) - Learn combinations of features\n",
    "    model.add(Dense(num_classes, activation='softmax')) # Output class probabilities (digits 0–9)\n",
    "\n",
    "    # Compile model with specified learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae14906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(X, y, input_shape, num_classes, filters1=32, filters2=64, dense_units=120, lr=0.001, epochs=5, batch_size=128):\n",
    "    \n",
    "    # Performs 5-fold stratified cross-validation on the training set using create_model().\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # y needs to be integers for StratifiedKFold\n",
    "    y_integers = np.argmax(y, axis=1)\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_integers), 1):\n",
    "        \n",
    "        print(f\"\\n##### Fold {fold} #####\")\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Build a fresh model for each fold\n",
    "        model = create_model(input_shape, num_classes, filters1=filters1, filters2=filters2, dense_units=dense_units, lr=lr)\n",
    "        \n",
    "        # Early stopping to avoid overfitting\n",
    "        early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "        \n",
    "        # Train\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stop]\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation fold\n",
    "        val_loss, val_acc = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        fold_accuracies.append(val_acc)\n",
    "    \n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"\\nAverage Validation Accuracy across 5 folds: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    return fold_accuracies, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df984e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter Search ###\n",
    "\n",
    "def hyperparameter_search(X_train, y_train, input_shape, num_classes):\n",
    "    filter_sets = [(16, 32), (32, 64), (64, 128)]\n",
    "    learning_rates = [0.001, 0.0005]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for filters1, filters2 in filter_sets:\n",
    "        for lr in learning_rates:\n",
    "\n",
    "            print(f\"\\n##### Testing filters: ({filters1}, {filters2}), lr: {lr} #####\")\n",
    "            fold_accuracies, avg_accuracy = cross_validate_model(\n",
    "                X_train, y_train, input_shape, num_classes,\n",
    "                filters1=filters1,\n",
    "                filters2=filters2,\n",
    "                dense_units=120,\n",
    "                lr=lr,\n",
    "                epochs=5,\n",
    "                batch_size=128\n",
    "            )\n",
    "            results.append({\n",
    "                'filters': (filters1, filters2),\n",
    "                'learning_rate': lr,\n",
    "                'fold_accuracies': fold_accuracies,\n",
    "                'avg_accuracy': avg_accuracy\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "results = hyperparameter_search(X_train, y_train, input_shape, num_classes)\n",
    "best_accuracy = 0\n",
    "best_result = None\n",
    "\n",
    "for result in results:\n",
    "    if result['avg_accuracy'] > best_accuracy:\n",
    "        best_accuracy = result['avg_accuracy']\n",
    "        best_result = result\n",
    "\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "print(f\"Filters: {best_result['filters']}\")\n",
    "print(f\"Learning rate: {best_result['learning_rate']}\")\n",
    "print(f\"Average validation accuracy: {best_result['avg_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building Best Model ###\n",
    "\n",
    "def train_final_model(X_train, y_train, X_test, y_test, input_shape, num_classes, best_result):\n",
    "\n",
    "    # Extract best hyperparameters\n",
    "    best_filters = best_result['filters']\n",
    "    best_lr = best_result['learning_rate']\n",
    "\n",
    "    print(f\"\\nTraining final model with hyperparameters:\")\n",
    "    print(f\"Filters: {best_filters}, Learning Rate: {best_lr}\")\n",
    "\n",
    "    # Create and compile model with best hyperparameters\n",
    "    model = create_model(\n",
    "        input_shape=input_shape,\n",
    "        num_classes=num_classes,\n",
    "        filters1=best_filters[0],\n",
    "        filters2=best_filters[1],\n",
    "        dense_units=120,\n",
    "        lr=best_lr\n",
    "    )\n",
    "\n",
    "    # Train on the full training data (60,000 images)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,              \n",
    "        batch_size=128,\n",
    "        validation_split=0.1,   # small part (10%) used for internal validation\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate on the test data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return model, history, test_accuracy\n",
    "\n",
    "best_model, history, final_test_accuracy = train_final_model(X_train, y_train, X_test, y_test, input_shape, num_classes, best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e720e03",
   "metadata": {},
   "source": [
    "#### My CNN Architecture\n",
    "\n",
    "The convolutional neural network (CNN) used in this project is based on the classic LeNet architecture, updated with modern techniques to improve performance on the MNIST-Digits dataset. It takes 28x28 grayscale images as input, where each pixel represents a level of brightness. The first layer is a convolutional layer with 64 filters of size 5x5. Think of these filters as small windows that scan the image to pick up simple patterns like edges or corners. The layer uses a ReLU activation, which helps the network learn more complex relationships by adding non-linearity. Next is a 2x2 max-pooling layer, which shrinks the image representation by keeping only the strongest signals in each small patch. This makes the model faster and more robust to small shifts or distortions in the image.\n",
    "\n",
    "The second convolutional layer has 128 filters of the same size, again followed by ReLU and 2x2 max-pooling. This layer captures more detailed patterns, like loops and intersections that appear in handwritten digits. After the convolutional layers, the output is flattened into a 1D vector so it can be fed into a dense layer with 120 neurons. Each neuron in this layer looks at combinations of features from the previous layers to make sense of the image. Finally, the output layer has 10 neurons with a softmax activation, which converts the network’s predictions into probabilities for each digit (0–9). The model is trained using the Adam optimizer with a learning rate of 0.0005 and categorical crossentropy loss, which measures how far the predicted probabilities are from the actual labels. Overall, this architecture achieves a high accuracy on MNIST while being easy to understand and computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d12ce6",
   "metadata": {},
   "source": [
    "#### Hyperparameter Search\n",
    "\n",
    "To find the best configuration for our CNN, a hyperparameter search was performed using 5-fold stratified cross-validation on the training set. Hyperparameters are settings that control how the network learns, such as the number of filters in each convolutional layer, which determine how many patterns the network can detect at each stage, and the learning rate, which controls how quickly the model updates its weights during training. In this project, three combinations of filters for the two convolutional layers were tested: 16 and 32, 32 and 64, and 64 and 128. Two learning rates, 0.001 and 0.0005, were explored while keeping the dense layer fixed at 120 neurons.\n",
    "\n",
    "For each combination, the training data was split into five folds in a stratified manner, meaning each fold maintained roughly the same proportion of each digit class. The model was trained on four folds and validated on the remaining fold. This process was repeated so that each fold was used as a validation set once. The performance of the model was measured by the average validation accuracy across the five folds. This approach ensures that the chosen hyperparameters are not specific to a particular subset of the training data and helps prevent overfitting, which happens when a model learns the training data too well but performs poorly on new data.\n",
    "\n",
    "The combination that achieved the highest average accuracy, specifically 64 and 128 filters with a learning rate of 0.0005, was chosen as the final configuration. Using cross-validation in this way helps guarantee that the selected hyperparameters generalize well to unseen data, providing a robust starting point for training the CNN on the full dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde9d34",
   "metadata": {},
   "source": [
    "#### Final Model Training & Test Accuracy\n",
    "\n",
    "After finding the best hyperparameters, specifically 64 filters in the first convolutional layer, 128 filters in the second, and a learning rate of 0.0005, the CNN was retrained on the full MNIST training dataset, which contains 60,000 images of handwritten digits. During this final training, the model went through 10 epochs, meaning it saw the entire training dataset 10 times, and it processed the images in small groups called batches of 128 images at a time to make learning more efficient. A small portion of the training data, 10%, was set aside as a validation set to check how well the model was learning and to ensure it was not just memorizing the training images. After training, the model was tested on the separate set of 10,000 unseen images, achieving a final test accuracy of 99.14%, which measures the proportion of digits the model classified correctly. This high accuracy shows that the chosen architecture and hyperparameters allow the network to generalize well, meaning it can correctly recognize new handwritten digits it has never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186ccf5",
   "metadata": {},
   "source": [
    "#### Comparison with Current MNIST Classification Results\n",
    "\n",
    "To contextualize this performance, it's valuable to compare it with some recent studies on MNIST digit classification:\n",
    "\n",
    "- Hyperparameter Tuning (Kundu et al., 2025): This research systematically explored the effect of hyperparameter tuning on CNN performance. By adjusting the learning rate, batch size, and number of convolutional layers, and using the Adam optimizer, their model achieved a recognition rate of 99.89%. The study highlighted that adding extra convolutional layers improved performance by allowing the network to extract deeper and more complex features. This is very similar to what we have tried to achieve in this lab.\n",
    "- Ensemble Methods: A study employing an ensemble of three simple CNN models with varying kernel sizes achieved a test accuracy of up to 99.87% on the MNIST dataset. This approach utilized data augmentation techniques, such as rotation and translation, to enhance model robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc8846",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "1. \"Introduction to Deep Learning.\" MIT Introduction to Deep Learning, Massachusetts Institute of Technology, 2024, https://introtodeeplearning.com/\n",
    "2. Kundu, Roumo, Anurag Sinha, Biresh Kumar, Rohan Gautam, Mohammad Shahid Raza, and Syed Abid Hussain. \"Exploration of Hyperparameter Tuning in Handwritten Digit Recognition Datasets Using CNN.\" F1000Research 14 (2025): 274. https://doi.org/10.12688/f1000research.161053.1\n",
    "3. An, S., Lee, M., Park, S., Yang, H., & So, J. (2020). An ensemble of simple convolutional neural network models for MNIST digit recognition (Version 2). arXiv. https://doi.org/10.48550/arXiv.2008.10400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
